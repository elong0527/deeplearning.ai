#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Notes for Andrew NG's Deep Learning Course 
\end_layout

\begin_layout Section
Notation
\end_layout

\begin_layout Itemize
\begin_inset Formula $\{(x^{(i)},y^{(i)}\}_{i=1,\dots m}$
\end_inset

: observed data
\end_layout

\begin_layout Itemize
\begin_inset Formula $a=\hat{y}=\sigma(w^{T}x+b)$
\end_inset

, where 
\begin_inset Formula $\sigma(z)=\frac{1}{1+e^{-z}}$
\end_inset

 is a sigmoid function
\end_layout

\begin_layout Itemize
Logistic Loss (error) function
\begin_inset Formula 
\[
\mathcal{L}(a,y)=-\left(y\log a+(1-y)\log(1-a)\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Logistic Cost function
\begin_inset Formula 
\[
J(w,b)=\frac{1}{m}\sum_{i=1}^{m}\mathcal{L}(a^{(i)},y^{(i)})
\]

\end_inset


\end_layout

\begin_layout Itemize
Logistic regression back propagation
\begin_inset Formula 
\[
dz=\frac{d\mathcal{L}}{dz}=a-y
\]

\end_inset


\begin_inset Formula 
\[
da=\frac{d\mathcal{L}}{da}=-\frac{y}{a}+\frac{1-y}{1-a}
\]

\end_inset


\begin_inset Formula 
\[
\frac{da}{dz}=a(1-a)
\]

\end_inset


\end_layout

\begin_layout Itemize
Vectorization:
\begin_inset Formula 
\[
A=(a^{(1)},\dots,a^{(m)})
\]

\end_inset


\begin_inset Formula 
\[
Z=(z^{(1)},\dots,z^{(m)})
\]

\end_inset


\begin_inset Formula 
\[
Y=(y^{(1)},\dots,y^{(m)})
\]

\end_inset


\begin_inset Formula 
\[
dZ=A-Y
\]

\end_inset


\end_layout

\begin_layout Subsection
Algorithm Convention
\end_layout

\begin_layout Subsection*
Sizes:
\end_layout

\begin_layout Itemize
\begin_inset Formula $m$
\end_inset

: number of observations
\end_layout

\begin_layout Itemize
\begin_inset Formula $n_{x}=n_{h}^{[L]}:$
\end_inset

 number of input features
\end_layout

\begin_layout Itemize
\begin_inset Formula $n_{y}=n_{h}^{[0]}$
\end_inset

: number of output features
\end_layout

\begin_layout Itemize
\begin_inset Formula $n_{h}^{[l]}$
\end_inset

, 
\begin_inset Formula $l=0,\dots,L$
\end_inset

: number of neurons at the 
\begin_inset Formula $l$
\end_inset

th layer
\end_layout

\begin_layout Itemize
\begin_inset Formula $L$
\end_inset

: number of layer
\end_layout

\begin_layout Subsection*
Input:
\end_layout

\begin_layout Itemize
\begin_inset Formula $X\in\mathbb{R}^{n_{x}\times m}$
\end_inset

: input matrix
\end_layout

\begin_layout Itemize
\begin_inset Formula $x^{(i)}\in\mathbb{R}^{n_{x}}$
\end_inset

: input matrix for 
\begin_inset Formula $i$
\end_inset

th subject
\end_layout

\begin_layout Itemize
\begin_inset Formula $Y\in\mathbb{R}^{n_{y}\times m}$
\end_inset

: output matrix
\end_layout

\begin_layout Itemize
\begin_inset Formula $y^{(i)}\in\mathbb{R}^{n_{y}}$
\end_inset

: output matrix for ith subject
\end_layout

\begin_layout Itemize
\begin_inset Formula $y$
\end_inset

, 
\begin_inset Formula $\hat{y}\in\mathbb{R}^{n_{y}}$
\end_inset

: population level output vector
\end_layout

\begin_layout Subsection*
Algorithm
\end_layout

\begin_layout Itemize
\begin_inset Formula $W^{[l]}\in\mathbb{R}^{[n_{h}^{[l]},n_{h}^{[l-1]}]}$
\end_inset

: 
\begin_inset Formula $i$
\end_inset

th layer weight matrix
\end_layout

\begin_layout Itemize
\begin_inset Formula $b^{[l]}\in\mathbb{R}^{[n_{h}^{[l]}]}$
\end_inset

: 
\begin_inset Formula $i$
\end_inset

th layer bias vector
\end_layout

\begin_layout Itemize
\begin_inset Formula $A^{[l]}\in\mathcal{\mathbb{R}}^{n\times m}$
\end_inset

: Activation matrix for 
\begin_inset Formula $l$
\end_inset

th layer
\end_layout

\begin_layout Itemize
\begin_inset Formula $a_{k}^{[i](j)}$
\end_inset

, 
\begin_inset Formula $i=1,\dots,n$
\end_inset

, 
\begin_inset Formula $j=1\dots,m$
\end_inset

 and 
\begin_inset Formula $k=1,\dots,K$
\end_inset

: 
\begin_inset Formula $k$
\end_inset

th neuron at 
\begin_inset Formula $i$
\end_inset

th layer for 
\begin_inset Formula $j$
\end_inset

th iteration.
\end_layout

\begin_layout Section
Method
\end_layout

\begin_layout Itemize
Gradient Descent
\end_layout

\begin_deeper
\begin_layout Itemize
Objective: 
\begin_inset Formula $\text{arg}\text{min}_{w,b}J(w,b)$
\end_inset


\end_layout

\begin_layout Itemize
Algorithm: 
\begin_inset Formula $w:=w-\alpha\frac{\partial J}{\partial w}=w-\alpha dw$
\end_inset

; 
\begin_inset Formula $b:=b-\alpha db$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Backpropagation
\end_layout

\begin_deeper
\begin_layout Itemize
http://colah.github.io/posts/2015-08-Backprop/
\end_layout

\begin_layout Itemize
http://neuralnetworksanddeeplearning.com/chap2.html
\end_layout

\begin_layout Itemize
http://www.math.uiuc.edu/documenta/vol-ismp/52_griewank-andreas-b.pdf
\end_layout

\end_deeper
\begin_layout Itemize
Gradiaent Descent for logistic regression with backpropagation
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $A^{[l]}=g(Z^{[l]})$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $dZ^{[L]}=A^{[L]}-Y$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $dZ^{[l]}=W^{[l+1]T}dZ^{[l+1]}*g^{[1]'}(Z^{[l]})$
\end_inset

; where 
\begin_inset Formula $l<L$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]^{T}}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $db=\frac{1}{m}\sum_{i=1}^{m}dZ_{i}^{[l]}$
\end_inset


\end_layout

\end_deeper
\end_body
\end_document
